{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup library imports\n",
    "import io, time, json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Documentation\n",
    "\n",
    "* Standard Library: \n",
    "    * [io](https://docs.python.org/2/library/io.html)\n",
    "    * [time](https://docs.python.org/2/library/time.html)\n",
    "    * [json](https://docs.python.org/2/library/json.html)\n",
    "\n",
    "* Third Party\n",
    "    * [requests](http://docs.python-requests.org/en/master/)\n",
    "    * [Beautiful Soup (version 4)](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "    * [yelp-fusion](https://www.yelp.com/developers/documentation/v3/get_started)\n",
    "\n",
    "**Note:** You may come across a `yelp-python` library online. The library is deprecated and incompatible with the current Yelp API, so do not use the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the homework on web scraping. While many people might view working with data (including scraping, parsing, storing, etc.) a necessary evil to get to the \"fun\" stuff (i.e. modeling), I think that if presented in the right way this munging can be quite empowering. Imagine you never had to worry or ask those _what if_ questions about data existing or being accessible... but that you can get it yourself!\n",
    "\n",
    "By the end of this exercise hopefully you should look at the wonderful world wide web without fear, comforted by the fact that anything you can see with your human eyes, a computer can see with its computer eyes...\n",
    " \n",
    "### Objectives\n",
    "\n",
    "But more concretely, this homework will teach you (and test you on):\n",
    "\n",
    "* HTTP Requests (and lifecycle)\n",
    "* RESTful APIs\n",
    "    * Authentication (OAuth)\n",
    "    * Pagination\n",
    "    * Rate limiting\n",
    "* JSON vs. HTML (and how to parse each)\n",
    "* HTML traversal (CSS selectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with APIs\n",
    "\n",
    "Since everyone loves food (presumably), the ultimate end goal of this homework will be to acquire the data to answer some questions and hypotheses about the restaurant scene in Pittsburgh (which we will get to later). We will download __both__ the metadata on restaurants in Pittsburgh from the Yelp API and with this metadata, retrieve the comments/reviews and ratings from users on restaurants.\n",
    "\n",
    "But first things first, let's do the \"hello world\" of making web requests with Python to get a sense for how to programmatically access web pages: an (unauthenticated) HTTP GET to download a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q0: Basic HTTP Requests\n",
    "\n",
    "Fill in the funtion to use `requests` to download and return the raw HTML content of the URL passed in as an argument. As an example try the following NYT article (on Facebook's algorithmic news feed): [http://www.nytimes.com/2016/08/28/magazine/inside-facebooks-totally-insane-unintentionally-gigantic-hyperpartisan-political-media-machine.html](http://www.nytimes.com/2016/08/28/magazine/inside-facebooks-totally-insane-unintentionally-gigantic-hyperpartisan-political-media-machine.html)\n",
    "\n",
    "> Your function should return a tuple of: (`<status_code>`, `<raw_html>`)\n",
    "\n",
    "```python\n",
    ">>> facebook_article = retrieve_html('http://www.nytimes.com/2016/08/28/magazine/inside-facebooks-totally-insane-unintentionally-gigantic-hyperpartisan-political-media-machine.html')\n",
    ">>> print(facebook_article)\n",
    "(200, u'<!DOCTYPE html>\\n<!--[if (gt IE 9)|!(IE)]> <!--> <html lang=\"en\" class=\"no-js section-magazine...')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_html(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string): \n",
    "\n",
    "    Returns:\n",
    "        status_code (integer):\n",
    "        raw_html (string): the raw HTML content of the response, properly encoded according to the HTTP headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write solution here\n",
    "    response = requests.get(url)\n",
    "    return (response.status_code, response.text)\n",
    "\n",
    "# facebook_article = retrieve_html('http://www.nytimes.com/2016/08/28/magazine/inside-facebooks-totally-insane-unintentionally-gigantic-hyperpartisan-political-media-machine.html')\n",
    "# print(facebook_article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now while this example might have been fun, we haven't yet done anything more than we could with a web browser. To really see the power of programmatically making web requests we will need to interact with a API. For the rest of this homework we will be working with the [Yelp API](https://www.yelp.com/developers/documentation/v3/get_started) and Yelp data (for an extensive data dump see their [Academic Dataset Challenge](https://www.yelp.com/dataset_challenge)). The reasons for using the Yelp API are 3 fold:\n",
    "\n",
    "1. Incredibly rich dataset that combines:\n",
    "    * entity data (users and businesses)\n",
    "    * preferences (i.e. ratings)\n",
    "    * geographic data (business location and check-ins)\n",
    "    * temporal data\n",
    "    * text in the form of reviews\n",
    "    * and even images.\n",
    "2. Well [documented API](https://www.yelp.com/developers/documentation/v3/get_started) with thorough examples.\n",
    "3. Extensive data coverage so that you can find data that you know personally (from your home town/city or account). This will help with understanding and interpreting your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "To access the Yelp API however we will need to go through a few more steps than we did with the first NYT example. Most large web scale companies use a combination of authentication and rate limiting to control access to their data to ensure that everyone using it abides. The first step (even before we make any request) is to setup a Yelp account if you do not have one and get API credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp API Access\n",
    "\n",
    "1. Create a Yelp account (if you do not have one already)\n",
    "2. [Generate API keys](https://www.yelp.com/developers/v3/manage_app) (if you haven't already). You will only need the API Key (not the Client ID or Client Secret) -- more on that later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our accounts setup we can start making requests! There are various authentication schemes that APIs use, listed here in relative order of complexity:\n",
    "\n",
    "* No authentication\n",
    "* [HTTP basic authentication](https://en.wikipedia.org/wiki/Basic_access_authentication)\n",
    "* Cookie based user login\n",
    "* OAuth (v1.0 & v2.0, see this [post](http://stackoverflow.com/questions/4113934/how-is-oauth-2-different-from-oauth-1) explaining the differences)\n",
    "* API keys\n",
    "* Custom Authentication\n",
    "\n",
    "For the NYT example, since it is a publicly visible page we did not need to authenticate. HTTP basic authentication isn't too common for consumer sites/applications that have the concept of user accounts (like Facebook, LinkedIn, Twitter, etc.) but is simple to setup quickly and you often encounter it on with individual password protected pages/sites. I'm sure you have seen this before somewhere:\n",
    "\n",
    "![http-basic](http://i.stack.imgur.com/QnUZW.png)\n",
    "\n",
    "Cookie based user login is what the majority of services use when you login with a browser (i.e. username and password). Once you sign in to a service like Facebook, the response stores a cookie in your browser to remember that you have logged in (HTTP is stateless). Each subsequent request to the same domain (i.e. any page on `facebook.com`) also sends the cookie that contains the authentication information to remind Facebook's servers that you have already logged in.\n",
    "\n",
    "Many REST APIs however use OAuth (authentication using tokens) which can be thought of a programmatic way to \"login\" _another_ user. Using tokens, a user (or application) only needs to send the login credentials once in the initial authentication and as a response from the server gets a special signed token. This signed token is then sent in future requests to the server (in place of the user credentials).\n",
    "\n",
    "A similar concept common used by many APIs is to assign API Keys to each client that needs access to server resources. The client must then pass the API Key along with _every_ request it makes to the API to authenticate. This is because the server is typically relatively stateless and does not maintain a session between subsequent calls from the same client. Most APIs (including Yelp) allow you to pass the API Key via a special HTTP Header: \"Authorization: Bearer <API_KEY>\". Check out the [docs](https://www.yelp.com/developers/documentation/v3/authentication) for more information.\n",
    "\n",
    "Yelp used to use OAuth tokens but has now switched to API Keys. **For the sake of backwards compatibility Yelp still provides a Client ID and Secret for OAuth, but you will not need those for this assignment.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Authenticated HTTP Request with the Yelp API\n",
    "\n",
    "First, store your Yelp credentials in a local file (kept out of version control) which you can read in to authenticate with the API. This file can be any format/structure since you will fill in the function stub below.\n",
    "\n",
    "For example, you may want to store your key in a file called `api_key.txt` (run in terminal):\n",
    "```bash\n",
    "echo 'YELP_API_KEY' > api_key.txt\n",
    "```\n",
    "\n",
    "You can then read from the file using:\n",
    "```python\n",
    "with open('api_key.txt', 'r') as f:\n",
    "    api_key = f.read().replace('\\n','')\n",
    "    # use your api_key\n",
    "```\n",
    "\n",
    "**KEEP THE API KEY FILE PRIVATE AND OUT OF VERSION CONTROL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Yelp API, fill in the following function stub to make an authenticated request to the [search](https://www.yelp.com/developers/documentation/v3/business_search) endpoint.\n",
    "\n",
    "> As a test, search for businesses in Pittsburgh. You should find ~13400 total depending on when you search (but this will actually differ from the number of actual Business objects returned... more on this in the next section)\n",
    "\n",
    "When writing the python request, you'll need to pass in a custom header as well as a parameter. See \n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    ">>> api_key = read_api_key('api_key.txt')\n",
    ">>> num_records, data = yelp_search(api_key, 'Pittsburgh')\n",
    ">>> print(num_records)\n",
    "13400\n",
    ">>> print(list(map(lambda x: x['name'], data)))\n",
    "['Gaucho Parrilla Argentina', 'Randyland', 'Redhawk Coffee', 'Phipps Conservatory and Botanical Gardens', 'La Gourmandine Bakery & Pastry Shop', ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_api_key(filepath):\n",
    "    \"\"\"\n",
    "    Read the Yelp API Key from file.\n",
    "    \n",
    "    Args:\n",
    "        filepath (string): File containing API Key\n",
    "    Returns:\n",
    "        api_key (string): The API Key\n",
    "    \"\"\"\n",
    "    \n",
    "    # feel free to modify this function if you are storing the API Key differently\n",
    "    with open(filepath + 'api_key.txt', 'r') as f:\n",
    "        return f.read().replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_key.txt\n",
      "course_roster.csv\n",
      "examples.py\n",
      "handout\n",
      "handout.tar\n",
      "Python and Jupyter basics.ipynb\n",
      "recitation1.ipynb\n",
      "shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /d/code/practicalDS/dataCollection-01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_search(api_key, query):\n",
    "    \"\"\"\n",
    "    Make an authenticated request to the Yelp API.\n",
    "\n",
    "    Args:\n",
    "        query (string): Search term\n",
    "\n",
    "    Returns:\n",
    "        total (integer): total number of businesses on Yelp corresponding to the query\n",
    "        businesses (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "    \"authorization\": 'Bearer %s' % api_key, # for the yelp API \n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"term\": query,\n",
    "        \"location\": \"Pittsburgh\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\"https://api.yelp.com/v3/businesses/search\",headers = headers, params = params)\n",
    "#     print(type(response.text))\n",
    "    result = response.json()\n",
    "#     print(type(result))\n",
    "#     return (result[\"total\"], result[\"businesses\"])\n",
    "    return response.text\n",
    "\n",
    "api_key= read_api_key(\"../\")\n",
    "result = yelp_search(api_key, 'restaurants')\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed the \"hello world\" of working with the Yelp API, we are ready to really fly! The rest of the exercise will have a bit less direction since there are a variety of ways to retrieve the requested information but you should have all the component knowledge at this point to work with the API. Yelp being a fairly general platform actually has many more business than just restaurants, but by using the flexibility of the API we can ask it to only return the restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterization and Pagination\n",
    "\n",
    "And before we can get any reviews on restaurants, we need to actually get the metadata on ALL of the restaurants in Pittsburgh. Notice above that while Yelp told us that there are ~13400, the response contained far fewer actual `Business` objects. This is due to pagination and is a safeguard against returning __TOO__ much data in a single request (what would happen if there were 100,000 restaurants?) and can be used in conjuction with _rate limiting_ as well as a way to throttle and protect access to Yelp data.\n",
    "\n",
    "> If an API has 1,000,000 records, but only returns 10 records per page and limits you to 5 requests per second... how long will it take to acquire ALL of the records contained in the API?\n",
    "\n",
    "One of the ways that APIs are an improvement over plain web scraping is the ability to make __parameterized__ requests. Just like the Python functions you have been writing have arguments (or parameters) that allow you to customize its behavior/actions (an output) without having to rewrite the function entirely, we can parameterize the queries we make to the Yelp API to filter the results it returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Aquire all of the restaurants in Pittsburgh (on Yelp)\n",
    "\n",
    "Again using the [API documentation](https://www.yelp.com/developers/documentation/v3/business_search) for the `search` endpoint, fill in the following function to retrieve all of the _Restuarants_ (using categories) for a given query. Again you should use your `read_api_key()` function outside of the `all_restaurants()` stub to read the API Key used for the requests. You will need to account for __pagination__ and __[rate limiting](https://www.yelp.com/developers/faq)__ to:\n",
    "\n",
    "1. Retrieve all of the Business objects (# of business objects should equal `total` in the response). Paginate by querying 20 restaurants each request.\n",
    "2. Pause slightly (at least 200 milliseconds) between subsequent requests so as to not overwhelm the API (and get blocked).  \n",
    "\n",
    "As always with API access, make sure you follow all of the [API's policies](https://www.yelp.com/developers/api_terms) and use the API responsibly and respectfully.\n",
    "\n",
    "** DO NOT MAKE TOO MANY REQUESTS TOO QUICKLY OR YOUR KEY MAY BE BLOCKED **\n",
    "\n",
    "> Again, you can test your function with an individual neighborhod in Pittsburgh (I recommend Polish Hill). Pittsburgh itself has a lot of restaurants... meaning it will take a lot of time to download them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> data = all_restaurants(api_key, 'Polish Hill, Pittsburgh')\n",
    ">>> print(len(data))\n",
    "41\n",
    ">>> print(data)\n",
    "['Lili Cafe', 'Morcilla', 'Umami', 'Piccolo Forno', \"Alfred's Deli & Market\", ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "def all_restaurants(api_key, query):\n",
    "    \"\"\"\n",
    "    Retrieve ALL the restaurants on Yelp for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (string): Search term\n",
    "\n",
    "    Returns:\n",
    "        results (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "    \"authorization\": 'Bearer %s' % api_key, # for the yelp API \n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"term\": \"Restuarants\",\n",
    "        \"location\": query,\n",
    "        \"limit\": 20,\n",
    "        \"offset\": offset\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\"https://api.yelp.com/v3/businesses/search\",headers = headers, params = params)\n",
    "    res = response.json()\n",
    "    result = []\n",
    "    for business in res['businesses']:\n",
    "        result.append(business)\n",
    "    print(\"result = {}\".format(result))\n",
    "    return result\n",
    "    \n",
    "\n",
    "import time\n",
    "results = []\n",
    "# api_key = read_api_key(\"../\")\n",
    "for i in range(2):\n",
    "    result = all_restaurants(api_key, 'Pittsburgh')\n",
    "    offset += 20\n",
    "    time.sleep(0.5)\n",
    "    if result is not None:\n",
    "        results.extend(result)\n",
    "print(\"results len = {}\".format(len(results)))\n",
    "print(\"results = {}\".format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the metadata on all of the restaurants in Pittsburgh (or at least the ones listed on Yelp), we can retrieve the reviews and ratings. The Yelp API gives us aggregate information on ratings but it doesn't give us the review text or individual users' ratings for a restaurant. For that we need to turn to web scraping, but to find out what pages to scrape we first need to parse our JSON from the API to extract the URLs of the restaurants.\n",
    "\n",
    "In general, it is a best practice to seperate the act of __downloading__ data and __parsing__ data. This ensures that your data processing pipeline is modular and extensible (and autogradable ;). This decoupling also solves the problem of expensive downloading but cheap parsing (in terms of computation and time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 2.5: Parse the API Responses and Extract the URLs\n",
    "\n",
    "Because we want to seperate the __downloading__ from the __parsing__, fill in the following function to parse the URLs pointing to the restaurants on `yelp.com`. As input your function should expect a string of [properly formatted JSON](http://www.json.org/) (which is similar to __BUT__ not the same as a Python dictionary) and as output should return a Python list of strings. The input JSON will be structured as follows (same as the [sample](https://www.yelp.com/developers/documentation/v3/business_search) on the Yelp API page):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"total\": 8228,\n",
    "  \"businesses\": [\n",
    "    {\n",
    "      \"rating\": 4,\n",
    "      \"price\": \"$\",\n",
    "      \"phone\": \"+14152520800\",\n",
    "      \"id\": \"four-barrel-coffee-san-francisco\",\n",
    "      \"is_closed\": false,\n",
    "      \"categories\": [\n",
    "        {\n",
    "          \"alias\": \"coffee\",\n",
    "          \"title\": \"Coffee & Tea\"\n",
    "        }\n",
    "      ],\n",
    "      \"review_count\": 1738,\n",
    "      \"name\": \"Four Barrel Coffee\",\n",
    "      \"url\": \"https://www.yelp.com/biz/four-barrel-coffee-san-francisco\",\n",
    "      \"coordinates\": {\n",
    "        \"latitude\": 37.7670169511878,\n",
    "        \"longitude\": -122.42184275\n",
    "      },\n",
    "      \"image_url\": \"http://s3-media2.fl.yelpcdn.com/bphoto/MmgtASP3l_t4tPCL1iAsCg/o.jpg\",\n",
    "      \"location\": {\n",
    "        \"city\": \"San Francisco\",\n",
    "        \"country\": \"US\",\n",
    "        \"address2\": \"\",\n",
    "        \"address3\": \"\",\n",
    "        \"state\": \"CA\",\n",
    "        \"address1\": \"375 Valencia St\",\n",
    "        \"zip_code\": \"94103\"\n",
    "      },\n",
    "      \"distance\": 1604.23,\n",
    "      \"transactions\": [\"pickup\", \"delivery\"]\n",
    "    }\n",
    "  ],\n",
    "  \"region\": {\n",
    "    \"center\": {\n",
    "      \"latitude\": 37.767413217936834,\n",
    "      \"longitude\": -122.42820739746094\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_list = ['https://www.yelp.com/biz/meat-and-potatoes-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/gaucho-parrilla-argentina-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/butcher-and-the-rye-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/t%C3%A4k%C5%8D-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/morcilla-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/altius-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/smallman-galley-pittsburgh-2?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/poulet-bleu-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/kaya-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/dianoias-eatery-pittsburgh-2?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/sienna-mercato-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/noodlehead-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/eleven-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/proper-brick-oven-and-tap-room-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/bakersfield-pittsburgh-8?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/vue-412-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/cure-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/musa-caribbean-cajun-fare-pittsburgh-2?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/sienna-on-the-square-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A', 'https://www.yelp.com/biz/the-twisted-frenchman-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A']\n"
     ]
    }
   ],
   "source": [
    "def parse_api_response(data):\n",
    "    \"\"\"\n",
    "    Parse Yelp API results to extract restaurant URLs.\n",
    "    \n",
    "    Args:\n",
    "        data (string): String of properly formatted JSON.\n",
    "\n",
    "    Returns:\n",
    "        (list): list of URLs as strings from the input JSON.\n",
    "    \"\"\"\n",
    "    \n",
    "    resp = json.loads(data)\n",
    "#     print(resp)\n",
    "    url = []\n",
    "    for business in resp['businesses']:\n",
    "        url.append(business['url'])\n",
    "    return url\n",
    "\n",
    "api_key= read_api_key(\"../\")\n",
    "sample_input = yelp_search(api_key, 'restaurants')\n",
    "url_list = parse_api_response(sample_input)\n",
    "print(\"url_list = {}\".format(url_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, JSON is quite trivial to parse (which is not the case with HTML as we will see in a second) and work with programmatically. This is why it is one of the most ubiquitous data serialization formats (especially for ReSTful APIs) and a huge benefit of working with a well defined API if one exists. But APIs do not always exists or provide the data we might need, and as a last resort we can always scrape web pages..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Web Pages (and HTML)\n",
    "\n",
    "Think of APIs as similar to accessing a application's database itself (something you can interactively query and receive structured data back). But the results are usually in a somewhat raw form with no formatting or visual representation (like the results from a database query). This is a benefit _AND_ a drawback depending on the end use case. For data science and _programatic_ analysis this raw form is quite ideal, but for an end user requesting information from a _graphical interface_ (like a web browser) this is very far from ideal since it takes some cognitive overhead to interpret the raw information. And vice versa, if we have HTML it is quite easy for a human to visually interpret it, but to try to perform some type of programmatic analysis we first need to parse the HTML into a more structured form.\n",
    "\n",
    "> As a general rule of thumb, if the data you need can be accessed or retrieved in a structured form (either from a bulk download or API) prefer that first. But if the data you want (and need) is not as in our case we need to resort to alternative (messier) means.\n",
    "\n",
    "Going back to the \"hello world\" example of question 1 with the NYT, we will do something similar to retrieve the HTML of the Yelp site itself (rather than going through the API) programmatically as text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Parse a Yelp restaurant Page\n",
    "\n",
    "Using `BeautifulSoup`, parse the HTML of a single Yelp restaurant page to extract the reviews in a structured form as well as the URL to the next page of reviews (or `None` if it is the last page). Fill in following function stubs to parse a single page of reviews and return:\n",
    "* the reviews as a structured Python dictionary\n",
    "* the HTML element containing the link/url for the next page of reviews (or None).\n",
    "\n",
    "For each review be sure to structure your Python dictionary as follows (to be graded correctly). The order of the keys doesn't matter, only the keys and the data type of the values:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'review_id': str\n",
    "    'user_id': str\n",
    "    'rating': float\n",
    "    'date': str ('yyyy-mm-dd')\n",
    "    'text': str\n",
    "}\n",
    "\n",
    "# Example\n",
    "{\n",
    "    'review_id': '12345'\n",
    "    'user_id': '6789'\n",
    "    'rating': 4.7\n",
    "    'date': '2016-01-23'\n",
    "    'text': \"Wonderful!\"\n",
    "}\n",
    "```\n",
    "\n",
    "> There can be issues with Beautiful Soup using various parsers, for maximum conpatibility (and fewest errors) initialize the library with the default (and Python standard library parser): `BeautifulSoup(markup, \"html.parser\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[{'review_id': 'PrDm9ly8prullA0fcDw4tQ', 'user_id': 'user_id:HgeE8guC565OELCyWLmY6w', 'rating': 4.0, 'date': '07/08/2018', 'text': \"DON'T CRY FOR ME ARGENTINAaAaAaAaA !I finally tried this Argentine chacusary styled restaurant. There is a line outside so you know something is up.See, the way it works is you order your food from the chalkboard and pay for it, collect your drinks, sit down and get served one time only. You have to walk around and find your own water and drinks but they still want a tip! I ordered the Carne steak sandwich with roasted peppers and I chose sirloin meat. I ordered the side of rosemary garlic clove potatoes which I ate with my sandwich which boosted the flavor. Everything is slightly charred.It was fun to try different styled food.\"}, {'review_id': 'WxUaIrfku2kBMagaga4jkg', 'user_id': 'user_id:uaDGJ86VDIcoDF7Rv90xPQ', 'rating': 5.0, 'date': '07/01/2018', 'text': \"I think that Gaucho might be my favorite place to eat in Pittsburgh, which is a little strange; even though their new space is about 500% larger than their old one, it's still not a comfortable place to sit! \\xa0But the food is just *so good* that it doesn't matter. I will come back again and again. \\xa0It's not fine dining, but my favorite thing on the menu is the steak sandwich made with ribeye steak. \\xa0Nearly all of the food is cooked directly over their incomparable wood-fired grill. \\xa0The house-made authentically Argentinian sauces complete the sandwich, so don't be shy and be sure to try them all! \\xa0I can't decide if I like the pimenton (smoky red pepper) or ajo (garlic) better!If you'd looking for a side dish, I can't recommend the grilled veggies enough. \\xa0Who knew that something so simple could be so delicious? \\xa0Also: I say skip the paella. \\xa0It was fun to eat as a group, but it lacked so much in flavor.Arrive early or in the middle of the afternoon to beat the wait. \\xa0BYOB!\"}, {'review_id': 'YrJcIzzBpf03NBLeLXlXJA', 'user_id': 'user_id:U-FY_uZs1set1LFs7YL12w', 'rating': 5.0, 'date': '05/24/2018', 'text': \"Classic spot to feed your inner Carnivore. One of my favorite spots in Pittsburgh. Great place for dates or groups, possibly even special occasions (i.e. family dinner over graduation). My one annoyance is the loooong wait and cramped waiting area during busy times... The line will be outside wrapped around the corner. For example: Arrived on a Saturday at 8 PM and probably waited for an hour +. They don't take reservations unfortunately. HOWEVER, if you're willing to wait it out, it's completely WORTH it. Gaucho delivers on the best beef, sauces, and other specialties (shrimp, pork, sides, etc). You can order plates or sandwiches. Seriously an overwhelming number of options. I've found that for two people, ordering two small plates and a side (potatoes, green beans, etc) is the perfect size to split and leave with a full belly. I recently split the Cerdo (roast pork), small plate of NY strip (median rare), and side of potatoes. \\xa0ALL delicious. The strip was perfectly pink / red. The pork wasn't dry but tender and flavorful. The Camarones (shrimp plate) and Carne Sandwich are both equally worthy. I have nothing negative to say in regards to the quality of food here. Out of the sauces (which are a must have regardless of your order), I obsess over the Ajo-Garlic (roasted garlic) and classic Chimmichurri-Yummy (Argentinian steak sauce).BYOB\"}, {'review_id': 'yjTUw-HgfULsaKDLK451DQ', 'user_id': 'user_id:BjtJ3VkMOxV2Lan037AFuw', 'rating': 5.0, 'date': '05/13/2018', 'text': 'So unbelievably good!I had been to their previous location and loved the food, but the space was beyond small making getting your food to go the only viable option. I was super excited to return once I learned that they had moved a couple doors down into a much larger space with two floors worth of seating, but every time I tried to get there the line to get in was literally around the block. The day we went there was actually no wait so we hurried up and parked and made a beeline to the door.Once inside I was not only blown away by how much nicer the new space was, but by how much was added to the menu since we visited the original location. After about 10 minutes of staring at the menu board we were ready to order.I chose a Carne ribeye steak sandwich which features your choice of steak with chimmichurri sauce, grilled peppers and caramelized onions. My steak was cooked perfectly. The steak was tender, very juicy and full of flavor. My wife chose the Pollo sandwich which features grilled chicken, pickled red onions, portobello mushrooms and ajo sauce. She absolutely loved it. Our daughter chose the steak burger which features a half pound burger, chimmichurri sauce and your choice of two topping. She added bacon and blue cheese. She loved her burger as well.One thing that I have always loved about Gaucho is that they have always cooked whatever meat I have order just as I ask. It is so hard to get perfectly cooked meat now a days, unfortunately. Service was quick and very attentive. I also like the fact that there is an exit door on the second floor that makes it easy to avoid the line of waiting people downstairs when you are ready to leave.Be sure to try each of their four sauces. Chimmichurri \"Argentine Steak Sauce\" which has fresh parsley and oregano, garlic, pepper flakes, red wine vinegar and olive oil. Ajo which has roasted garlic cloves with caramelized onions, parsley and pepper with olive oil. Pimenton which has grilled red bell peppers, fresh garlic, parsley, vinegar and olive oil. And Cebolla which has caramelized onion, vinegar, olive oil and parsley. Each sauce is very unique and different and all worth trying.'}]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def parse_page(html):\n",
    "    \"\"\"\n",
    "    Parse the reviews on a single page of a restaurant.\n",
    "    \n",
    "    Args:\n",
    "        html (string): String of HTML corresponding to a Yelp restaurant\n",
    "\n",
    "    Returns:\n",
    "        tuple(list, string): a tuple of two elements\n",
    "            first element: list of dictionaries corresponding to the extracted review information\n",
    "            second element: URL for the next page of reviews (or None if it is the last page)\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(html)\n",
    "    print(response.status_code)\n",
    "    root = BeautifulSoup(response.text, \"html.parser\")\n",
    "    reviews = root.findAll(\"div\",{\"class\": \"review review--with-sidebar\"})\n",
    "    dict_list1 = []\n",
    "    dict_list2 = []\n",
    "    for review in reviews:\n",
    "        review_id = review[\"data-review-id\"]\n",
    "        user_id = review[\"data-signup-object\"]\n",
    "        dict1  = dict([(\"review_id\", review_id), (\"user_id\", user_id)])\n",
    "        dict_list1.append(dict1)\n",
    "    review_contents = root.findAll(\"div\",{\"class\": \"review-content\"})\n",
    "    for content in review_contents:\n",
    "        rating = float(content.div.div.div['title'].split(\" \")[0])\n",
    "        date = datetime.datetime.strptime(content.div.span.text.split('\\n')[1].lstrip(),'%m/%d/%Y').strftime(\"%m/%d/%Y\")\n",
    "        text = content.p.text\n",
    "        dict2  = dict([(\"rating\", rating), (\"date\", date), ('text', text)])\n",
    "        dict_list2.append(dict2)\n",
    "    result = []\n",
    "    for i in range(0, len(dict_list1)):\n",
    "        z = dict_list1[i]\n",
    "        z.update(dict_list2[i])\n",
    "        result.append(z)\n",
    "    print(result[1:5])\n",
    "    return result\n",
    "    \n",
    "    \n",
    "html = \"https://www.yelp.com/biz/gaucho-parrilla-argentina-pittsburgh?adjust_creative=nJbdqWu2t1WtpsRK8MuB_A&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=nJbdqWu2t1WtpsRK8MuB_A\"\n",
    "result = parse_page(html)\n",
    "\n",
    "# response = requests.get(html)\n",
    "# root = BeautifulSoup(response.text, \"html.parser\")\n",
    "# root.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.5: Extract all of the Yelp reviews for a Single Restaurant\n",
    "\n",
    "So now that we have parsed a single page, and figured out a method to go from one page to the next we are ready to combine these two techniques and actually crawl through web pages! \n",
    "\n",
    "Using `requests`, programmatically retrieve __ALL__ of the reviews for a __single__ restaurant (provided as a parameter). Just like the API was paginated, the HTML paginates its reviews (it would be a very long web page to show 300 reviews on a single page) and to get all the reviews you will need to parse and traverse the HTML. As input your function will receive a URL corresponding to a Yelp restaurant. As output return a list of dictionaries (structured the same as question 3) containing the relevant information from the reviews.\n",
    "\n",
    "```python\n",
    ">>> data = extract_reviews('https://www.yelp.com/biz/the-porch-at-schenley-pittsburgh')\n",
    ">>> print len(data)\n",
    "513\n",
    ">>> print data[0]\n",
    "{\n",
    "    'text': \"I've only had the pizza at the Porch, so this 4 stars is for the pizza! This is a great place to come, especially for their late-night half-off pizza special during the weekdays. I've looked at their non-pizza menu and it's a bit pricey, but from what other people tell me, the other food is great. The pizza is more than large enough to feed one person, but my friends and I usually split a couple between us so we can have different flavors. My favorites is the Piggie Pie, but I also had another seasonal pizza with goat cheese and fig on it, which was also extremely good! The crust is nice and crunchy on the outside, but soft enough on the inside, and the ratio of sauce-to-crust is perfect. Overall, this is a fabulous spot to come to if you're a student in Oakland looking for a late-night snack, or a non-student who wants a classier spot to eat at in Oakland. Will be back again!\", \n",
    "    'date': '12/22/2017', \n",
    "    'user_id': 'SoItWLyIQUKtp8_SvQRMFg', \n",
    "    'review_id': '5-Qhk9s94w7eHICF2Fhk7Q', \n",
    "    'rating': 4.0\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
